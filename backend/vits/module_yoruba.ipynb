{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m commons\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msubprocess\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from IPython.display import Audio\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import tempfile\n",
    "import math\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from . import commons\n",
    "import argparse\n",
    "import subprocess\n",
    "from . import utils\n",
    "import torchvision\n",
    "import io\n",
    "import librosa\n",
    "from fastapi.responses import Response\n",
    "#from data_utils import TextAudioLoader, TextAudioCollate, TextAudioSpeakerLoader, TextAudioSpeakerCollate\n",
    "from .models import SynthesizerTrn\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "ckpt_dir = \"./vits/yor\"\n",
    "LANG = \"yor\"\n",
    "\n",
    "def preprocess_char(text, lang=None):\n",
    "    \"\"\"\n",
    "    Special treatement of characters in certain languages\n",
    "    \"\"\"\n",
    "    print(lang)\n",
    "    if lang == 'ron':\n",
    "        text = text.replace(\"ț\", \"ţ\")\n",
    "    return text\n",
    "\n",
    "class TextMapper(object):\n",
    "    def __init__(self, vocab_file):\n",
    "        self.symbols = [x.replace(\"\\n\", \"\") for x in open(vocab_file, encoding=\"utf-8\").readlines()]\n",
    "        self.SPACE_ID = self.symbols.index(\" \")\n",
    "        self._symbol_to_id = {s: i for i, s in enumerate(self.symbols)}\n",
    "        self._id_to_symbol = {i: s for i, s in enumerate(self.symbols)}\n",
    "\n",
    "    def text_to_sequence(self, text, cleaner_names):\n",
    "        '''Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n",
    "        Args:\n",
    "        text: string to convert to a sequence\n",
    "        cleaner_names: names of the cleaner functions to run the text through\n",
    "        Returns:\n",
    "        List of integers corresponding to the symbols in the text\n",
    "        '''\n",
    "        sequence = []\n",
    "        clean_text = text.strip()\n",
    "        for symbol in clean_text:\n",
    "            symbol_id = self._symbol_to_id[symbol]\n",
    "            sequence += [symbol_id]\n",
    "        return sequence\n",
    "\n",
    "    def uromanize(self, text, uroman_pl):\n",
    "        iso = \"xxx\"\n",
    "        with tempfile.NamedTemporaryFile() as tf, \\\n",
    "             tempfile.NamedTemporaryFile() as tf2:\n",
    "            with open(tf.name, \"w\") as f:\n",
    "                f.write(\"\\n\".join([text]))\n",
    "            cmd = f\"perl \" + uroman_pl\n",
    "            cmd += f\" -l {iso} \"\n",
    "            cmd +=  f\" < {tf.name} > {tf2.name}\"\n",
    "            os.system(cmd)\n",
    "            outtexts = []\n",
    "            with open(tf2.name) as f:\n",
    "                for line in f:\n",
    "                    line =  re.sub(r\"\\s+\", \" \", line).strip()\n",
    "                    outtexts.append(line)\n",
    "            outtext = outtexts[0]\n",
    "        return outtext\n",
    "\n",
    "    def get_text(self, text, hps):\n",
    "        text_norm = self.text_to_sequence(text, hps.data.text_cleaners)\n",
    "        if hps.data.add_blank:\n",
    "            text_norm = commons.intersperse(text_norm, 0)\n",
    "        text_norm = torch.LongTensor(text_norm)\n",
    "        return text_norm\n",
    "\n",
    "    def filter_oov(self, text):\n",
    "        val_chars = self._symbol_to_id\n",
    "        txt_filt = \"\".join(list(filter(lambda x: x in val_chars, text)))\n",
    "        print(f\"text after filtering OOV: {txt_filt}\")\n",
    "        return txt_filt\n",
    "\n",
    "def preprocess_text(txt, text_mapper, hps, uroman_dir=None, lang=None):\n",
    "    txt = preprocess_char(txt, lang=lang)\n",
    "    is_uroman = hps.data.training_files.split('.')[-1] == 'uroman'\n",
    "    if is_uroman:\n",
    "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "            if uroman_dir is None:\n",
    "                cmd = f\"git clone git@github.com:isi-nlp/uroman.git {tmp_dir}\"\n",
    "                print(cmd)\n",
    "                subprocess.check_output(cmd, shell=True)\n",
    "                uroman_dir = tmp_dir\n",
    "            uroman_pl = os.path.join(uroman_dir, \"bin\", \"uroman.pl\")\n",
    "            print(f\"uromanize\")\n",
    "            txt = text_mapper.uromanize(txt, uroman_pl)\n",
    "            print(f\"uroman text: {txt}\")\n",
    "    txt = txt.lower()\n",
    "    txt = text_mapper.filter_oov(txt)\n",
    "    return txt\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Run inference with {device}\")\n",
    "vocab_file = f\"{ckpt_dir}/vocab.txt\"\n",
    "print(vocab_file)\n",
    "config_file = f\"{ckpt_dir}/config.json\"\n",
    "assert os.path.isfile(config_file), f\"{config_file} doesn't exist\"\n",
    "hps = utils.get_hparams_from_file(config_file)\n",
    "text_mapper = TextMapper(vocab_file)\n",
    "net_g = SynthesizerTrn(\n",
    "    len(text_mapper.symbols),\n",
    "    hps.data.filter_length // 2 + 1,\n",
    "    hps.train.segment_size // hps.data.hop_length,\n",
    "    **hps.model)\n",
    "net_g.to(device)\n",
    "_ = net_g.eval()\n",
    "\n",
    "# g_pth = \"https://drive.google.com/file/d/1gtp6JdblyslLt0ObLTHwWIAUQNSqNWk1/view?usp=sharing\"\n",
    "g_pth = f\"{ckpt_dir}/G_100000.pth\"\n",
    "print(f\"load {g_pth}\")\n",
    "\n",
    "_ = utils.load_checkpoint(g_pth, net_g, None)\n",
    "\n",
    "#txt = \"Ọmọ ẹgbẹ́ òkùnkùn dèrò àtìmọ́lé torí nílùú Ìbàdàn.\"\n",
    "\n",
    "#print(f\"text: {txt}\")\n",
    "# txt = preprocess_text(txt, text_mapper, hps, lang=LANG)\n",
    "# stn_tst = text_mapper.get_text(txt, hps)\n",
    "# with torch.no_grad():\n",
    "#     x_tst = stn_tst.unsqueeze(0).to(device)\n",
    "#     x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).to(device)\n",
    "#     hyp = net_g.infer(\n",
    "#         x_tst, x_tst_lengths, noise_scale=.667,\n",
    "#         noise_scale_w=0.8, length_scale=1.0\n",
    "#     )[0][0,0].cpu().float().numpy()\n",
    "\n",
    "# print(f\"Generated audio\")\n",
    "# # Generate or process the audio file\n",
    "# audio_data = Audio(hyp, rate=hps.data.sampling_rate)\n",
    "\n",
    "def text_to_audio(txt):\n",
    "    txt = preprocess_text(txt, text_mapper, hps, lang=LANG)\n",
    "    stn_tst = text_mapper.get_text(txt, hps)\n",
    "    with torch.no_grad():\n",
    "        x_tst = stn_tst.unsqueeze(0).to(device)\n",
    "        x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).to(device)\n",
    "        hyp = net_g.infer(\n",
    "            x_tst, x_tst_lengths, noise_scale=.667,\n",
    "            noise_scale_w=0.8, length_scale=1.0\n",
    "        )[0][0,0].cpu().float().numpy()\n",
    "\n",
    "    print(f\"Generated audio\")\n",
    "    print(hyp)\n",
    "    # Generate or process the audio file\n",
    "    # audio_data = Audio(hyp, rate=hps.data.sampling_rate)\n",
    "    # # Convert the blob to a file-like object\n",
    "    # # Convert the audio data to a bytes-like object\n",
    "    # audio_blob = librosa.core.audio.to_wav(audio_data)\n",
    "    # audio_file = io.BytesIO(audio_blob)\n",
    "    # audio_data = hyp.astype(np.int16)  # Convert to int16 if it's not\n",
    "    # audio_bytes = audio_data.tobytes()  # Convert to bytes\n",
    "    #return  Response(content=audio_bytes, media_type=\"audio/wav\")\n",
    "    \n",
    "    # Convert the generated audio data to the appropriate format (e.g., 16-bit PCM)\n",
    "    audio_data = (hyp * 32767).astype(np.int16)\n",
    "\n",
    "    # Encode the audio data as bytes\n",
    "    audio_bytes = audio_data.tobytes()\n",
    "\n",
    "    # Create a Response with the binary data and the appropriate media type\n",
    "    response = Response(content=audio_bytes, media_type=\"audio/wav\")\n",
    "    return response\n",
    "    #return hyp\n",
    "\n",
    "text_to_audio(\"emi ni\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
